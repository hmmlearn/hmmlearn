import numpy as np
import pytest

from hmmlearn import hmm

from . import assert_log_likelihood_increasing, normalized


class TestMultinomialAgainstWikipedia:
    """
    Examples from Wikipedia:

    - http://en.wikipedia.org/wiki/Hidden_Markov_model
    - http://en.wikipedia.org/wiki/Viterbi_algorithm
    """
    def setup(self):
        self.implementations = ["log", "scaling"]

    def new_hmm(self, impl):
        n_components = 2   # ['Rainy', 'Sunny']
        n_features = 3     # ['walk', 'shop', 'clean']
        h = hmm.MultinomialHMM(n_components, implementation=impl)
        h.n_features = n_features
        h.startprob_ = np.array([0.6, 0.4])
        h.transmat_ = np.array([[0.7, 0.3], [0.4, 0.6]])
        h.emissionprob_ = np.array([[0.1, 0.4, 0.5],
                                    [0.6, 0.3, 0.1]])
        return h

    def test_decode_viterbi(self):
        # From http://en.wikipedia.org/wiki/Viterbi_algorithm:
        # "This reveals that the observations ['walk', 'shop', 'clean']
        #  were most likely generated by states ['Sunny', 'Rainy', 'Rainy'],
        #  with probability 0.01344."
        for impl in self.implementations:
            h = self.new_hmm(impl)
            X = [[0], [1], [2]]
            logprob, state_sequence = h.decode(X, algorithm="viterbi")
            assert round(np.exp(logprob), 5) == 0.01344
            assert np.allclose(state_sequence, [1, 0, 0])

    def test_decode_map(self):
        X = [[0], [1], [2]]
        for impl in self.implementations:
            h = self.new_hmm(impl)
            _logprob, state_sequence = h.decode(X, algorithm="map")
            assert np.allclose(state_sequence, [1, 0, 0])

    def test_predict(self):
        X = [[0], [1], [2]]
        for impl in self.implementations:
            h = self.new_hmm(impl)
            state_sequence = h.predict(X)
            posteriors = h.predict_proba(X)
            assert np.allclose(state_sequence, [1, 0, 0])
            assert np.allclose(posteriors, [
                [0.23170303, 0.76829697],
                [0.62406281, 0.37593719],
                [0.86397706, 0.13602294],
            ])


class TestMultinomailHMM:
    def setup(self):
        self.implementations = ["log", "scaling"]
        self.n_components = 2
        self.n_features = 3

    def new_hmm(self, impl):
        h = hmm.MultinomialHMM(self.n_components, implementation=impl)
        h.startprob_ = np.array([0.6, 0.4])
        h.transmat_ = np.array([[0.7, 0.3], [0.4, 0.6]])
        h.emissionprob_ = np.array([[0.1, 0.4, 0.5], [0.6, 0.3, 0.1]])
        return h

    def test_attributes(self):
        for impl in self.implementations:
            with pytest.raises(ValueError):
                h = self.new_hmm(impl)
                h.emissionprob_ = []
                h._check()
            with pytest.raises(ValueError):
                h.emissionprob_ = np.zeros((self.n_components - 2,
                                            self.n_features))
                h._check()

    def test_score_samples(self):
        idx = np.repeat(np.arange(self.n_components), 10)
        n_samples = len(idx)
        X = np.random.randint(self.n_features, size=(n_samples, 1))
        for impl in self.implementations:
            h = self.new_hmm(impl)

            ll, posteriors = h.score_samples(X)
            assert posteriors.shape == (n_samples, self.n_components)
            assert np.allclose(posteriors.sum(axis=1), np.ones(n_samples))

    def test_sample(self, n_samples=1000):
        for impl in self.implementations:
            h = self.new_hmm(impl)
            X, state_sequence = h.sample(n_samples)
            assert X.ndim == 2
            assert len(X) == len(state_sequence) == n_samples
            assert len(np.unique(X)) == self.n_features

    def test_fit(self, params='ste', n_iter=5):

        for impl in self.implementations:
            h = self.new_hmm(impl)
            h.params = params

            lengths = np.array([10] * 10)
            X, _state_sequence = h.sample(lengths.sum())

            # Mess up the parameters and see if we can re-learn them.
            h.startprob_ = normalized(np.random.random(self.n_components))
            h.transmat_ = normalized(
                np.random.random((self.n_components, self.n_components)),
                axis=1)
            h.emissionprob_ = normalized(
                np.random.random((self.n_components, self.n_features)),
                axis=1)

        assert_log_likelihood_increasing(h, X, lengths, n_iter)

    def test_fit_emissionprob(self):
        self.test_fit('e')

    def test_fit_with_init(self, params='ste', n_iter=5):
        lengths = [10] * 10
        for impl in self.implementations:
            h = self.new_hmm(impl)
            X, _state_sequence = h.sample(sum(lengths))

            # use init_function to initialize paramerters
            h = hmm.MultinomialHMM(self.n_components, params=params,
                                   init_params=params)
            h._init(X, lengths=lengths)

        assert_log_likelihood_increasing(h, X, lengths, n_iter)

    def test__check_and_set_multinomial_n_features(self):
        for impl in self.implementations:
            h = self.new_hmm(impl)
            h._check_and_set_multinomial_n_features(
                np.array([[0, 0, 2, 1, 3, 1, 1]]))
            h._check_and_set_multinomial_n_features(
                np.array([[0, 0, 1, 3, 1]], np.uint8))
            with pytest.raises(ValueError):  # non-integral
                h._check_and_set_multinomial_n_features(
                    np.array([[0., 2., 1., 3.]]))
            with pytest.raises(ValueError):  # negative integers
                h._check_and_set_multinomial_n_features(
                    np.array([[0, -2, 1, 3, 1, 1]]))
